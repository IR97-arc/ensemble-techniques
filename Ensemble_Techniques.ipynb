{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "n914MUaYxn4v",
        "outputId": "738d5b7c-fa0a-4f70-9f4e-597676573aea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Q1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d95e5dbd2ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Practical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQ1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Q1' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#Practical\n",
        "Q1\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Initialize Bagging Classifier with DecisionTreeClassifier as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,  # Number of base estimators\n",
        "    max_samples=0.8,  # Fraction of samples to draw\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define accuracy scoring metric\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score)\n",
        "}\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "# Extract and print mean and standard deviation of accuracy scores\n",
        "accuracy_mean = np.mean(cv_results['test_accuracy'])\n",
        "accuracy_std = np.std(cv_results['test_accuracy'])\n",
        "\n",
        "print(\"Bagging Classifier (Decision Tree) Performance on Iris Dataset:\")\n",
        "print(f\"Accuracy: Mean = {accuracy_mean:.4f}, Std = {accuracy_std:.4f}\")\n",
        "Q2\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, mean_squared_error\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Initialize Bagging Regressor with DecisionTreeRegressor as base estimator\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=50,  # Number of base estimators\n",
        "    max_samples=0.8,  # Fraction of samples to draw\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define MSE scoring metric\n",
        "scoring = {\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "# Extract and print mean and standard deviation of MSE scores\n",
        "mse_mean = -np.mean(cv_results['test_mse'])  # Negate because greater_is_better=False\n",
        "mse_std = np.std(cv_results['test_mse'])\n",
        "\n",
        "print(\"Bagging Regressor (Decision Tree) Performance:\")\n",
        "print(f\"MSE: Mean = {mse_mean:.4f}, Std = {mse_std:.4f}\")\n",
        "Q3\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to show feature names and their importance\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(feature_df)\n",
        "\n",
        "Q4\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the models\n",
        "decision_tree_model = DecisionTreeRegressor(random_state=42)\n",
        "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Decision Tree model\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Train the Random Forest model\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with both models\n",
        "y_pred_tree = decision_tree_model.predict(X_test)\n",
        "y_pred_forest = random_forest_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for both models\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "mse_forest = mean_squared_error(y_test, y_pred_forest)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Decision Tree Model Mean Squared Error (MSE): {mse_tree}\")\n",
        "print(f\"Random Forest Model Mean Squared Error (MSE): {mse_forest}\")\n",
        "\n",
        "Q5\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (we'll use training for OOB score)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier with OOB enabled\n",
        "rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get the OOB score\n",
        "oob_score = rf_model.oob_score_\n",
        "\n",
        "print(f\"Out-of-Bag (OOB) Score: {oob_score}\")\n",
        "\n",
        "Q6\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Scale features (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize Bagging Classifier with SVC as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=SVC(probability=True, kernel='rbf', random_state=42),\n",
        "    n_estimators=20,  # Fewer estimators due to SVM's computational cost\n",
        "    max_samples=0.8,  # Fraction of samples to draw\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define accuracy scoring metric\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score)\n",
        "}\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "# Extract and print mean and standard deviation of accuracy scores\n",
        "accuracy_mean = np.mean(cv_results['test_accuracy'])\n",
        "accuracy_std = np.std(cv_results['test_accuracy'])\n",
        "\n",
        "print(\"Bagging Classifier (SVM) Performance:\")\n",
        "print(f\"Accuracy: Mean = {accuracy_mean:.4f}, Std = {accuracy_std:.4f}\")\n",
        "Q7\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List to store results\n",
        "n_trees = [10, 50, 100, 200, 500]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate Random Forest Classifiers with different numbers of trees\n",
        "for n in n_trees:\n",
        "    rf_model = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)  # Train the model\n",
        "    y_pred = rf_model.predict(X_test)  # Make predictions\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print the accuracy for each number of trees\n",
        "for n, accuracy in zip(n_trees, accuracies):\n",
        "    print(f\"Number of Trees: {n}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Plot the accuracy as a function of the number of trees\n",
        "plt.plot(n_trees, accuracies, marker='o')\n",
        "plt.title('Random Forest Accuracy with Different Numbers of Trees')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "Q8\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, roc_auc_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with LogisticRegression as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
        "    n_estimators=50,  # Number of base estimators\n",
        "    max_samples=0.8,  # Fraction of samples to draw\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define AUC scoring metric\n",
        "scoring = {\n",
        "    'auc': make_scorer(roc_auc_score, needs_proba=True)\n",
        "}\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "# Extract and print mean and standard deviation of AUC scores\n",
        "auc_mean = np.mean(cv_results['test_auc'])\n",
        "auc_std = np.std(cv_results['test_auc'])\n",
        "\n",
        "print(\"Bagging Classifier (Logistic Regression) Performance:\")\n",
        "print(f\"AUC Score: Mean = {auc_mean:.4f}, Std = {auc_std:.4f}\")\n",
        "Q9\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort features by importance\n",
        "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(feature_df)\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_df['Feature'], feature_df['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance Scores for Random Forest Regressor')\n",
        "plt.gca().invert_yaxis()  # To show the most important feature on top\n",
        "plt.show()\n",
        "\n",
        "Q10\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Bagging Classifier': BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(random_state=42),\n",
        "        n_estimators=50,  # Number of base estimators\n",
        "        max_samples=0.8,  # Fraction of samples to draw\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=50,  # Match number of estimators for fair comparison\n",
        "        max_features='sqrt',  # Random feature selection\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Define scoring metric\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score)\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    # Perform 5-fold cross-validation\n",
        "    cv_results = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "    # Store mean and std of accuracy\n",
        "    results[name] = {\n",
        "        'accuracy_mean': np.mean(cv_results['test_accuracy']),\n",
        "        'accuracy_std': np.std(cv_results['test_accuracy'])\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "print(\"Performance Comparison: Bagging Classifier vs. Random Forest\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Accuracy: Mean = {metrics['accuracy_mean']:.4f}, Std = {metrics['accuracy_std']:.4f}\")\n",
        "Q11\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],   # Number of trees\n",
        "    'max_depth': [10, 20, None],  # Maximum depth of the trees\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
        "\n",
        "# Train the Random Forest Classifier using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from GridSearchCV\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Test the model with the best hyperparameters\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Best Hyperparameters: {accuracy}\")\n",
        "\n",
        "Q12\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Define different numbers of base estimators\n",
        "n_estimators_values = [10, 20, 50, 100, 200]\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'r2': make_scorer(r2_score)\n",
        "}\n",
        "\n",
        "# Train and evaluate Bagging Regressor for each n_estimators value\n",
        "results = {}\n",
        "for n_estimators in n_estimators_values:\n",
        "    # Initialize Bagging Regressor\n",
        "    bagging = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=n_estimators,\n",
        "        max_samples=0.8,  # Fraction of samples to draw\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Perform 5-fold cross-validation\n",
        "    cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "    # Store mean and std of scores\n",
        "    results[n_estimators] = {\n",
        "        'mse_mean': -np.mean(cv_results['test_mse']),  # Negate because greater_is_better=False\n",
        "        'mse_std': np.std(cv_results['test_mse']),\n",
        "        'r2_mean': np.mean(cv_results['test_r2']),\n",
        "        'r2_std': np.std(cv_results['test_r2'])\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor Performance Comparison (Varying n_estimators):\")\n",
        "for n_estimators, metrics in results.items():\n",
        "    print(f\"\\nNumber of Estimators: {n_estimators}\")\n",
        "    print(f\"MSE: Mean = {metrics['mse_mean']:.4f}, Std = {metrics['mse_std']:.4f}\")\n",
        "    print(f\"R²: Mean = {metrics['r2_mean']:.4f}, Std = {metrics['r2_std']:.4f}\")\n",
        "Q13\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Identify the misclassified samples\n",
        "misclassified_idx = (y_pred != y_test)\n",
        "\n",
        "# Display the misclassified samples\n",
        "misclassified_samples = pd.DataFrame(X_test[misclassified_idx], columns=data.feature_names)\n",
        "misclassified_samples['True Class'] = y_test[misclassified_idx]\n",
        "misclassified_samples['Predicted Class'] = y_pred[misclassified_idx]\n",
        "\n",
        "# Print the misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_samples)\n",
        "\n",
        "\n",
        "Q14\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Bagging Classifier': BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(random_state=42),\n",
        "        n_estimators=50,  # Number of base estimators\n",
        "        max_samples=0.8,  # Fraction of samples to draw\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    # Perform 5-fold cross-validation\n",
        "    cv_results = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "    # Store mean and std of scores\n",
        "    results[name] = {\n",
        "        'precision_mean': np.mean(cv_results['test_precision']),\n",
        "        'precision_std': np.std(cv_results['test_precision']),\n",
        "        'recall_mean': np.mean(cv_results['test_recall']),\n",
        "        'recall_std': np.std(cv_results['test_recall']),\n",
        "        'f1_mean': np.mean(cv_results['test_f1']),\n",
        "        'f1_std': np.std(cv_results['test_f1'])\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "print(\"Performance Comparison: Bagging Classifier vs. Decision Tree\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print(f\"Precision: Mean = {metrics['precision_mean']:.4f}, Std = {metrics['precision_std']:.4f}\")\n",
        "    print(f\"Recall: Mean = {metrics['recall_mean']:.4f}, Std = {metrics['recall_std']:.4f}\")\n",
        "    print(f\"F1-Score: Mean = {metrics['f1_mean']:.4f}, Std = {metrics['f1_std']:.4f}\")\n",
        "Q15\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "plt.show()\n",
        "\n",
        "Q16\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(random_state=42)\n",
        "lr = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_model = StackingClassifier(estimators=[('dt', dt), ('svm', svm), ('lr', lr)], final_estimator=LogisticRegression())\n",
        "\n",
        "# Create the voting classifier (for comparison)\n",
        "voting_model = VotingClassifier(estimators=[('dt', dt), ('svm', svm), ('lr', lr)], voting='hard')\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Train the voting classifier\n",
        "voting_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict with both models\n",
        "y_pred_stacking = stacking_model.predict(X_test)\n",
        "y_pred_voting = voting_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for both models\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
        "\n",
        "# Print the accuracy results\n",
        "print(f\"Accuracy of Stacking Classifier: {accuracy_stacking * 100:.2f}%\")\n",
        "print(f\"Accuracy of Voting Classifier: {accuracy_voting * 100:.2f}%\")\n",
        "\n",
        "# Compare accuracies\n",
        "if accuracy_stacking > accuracy_voting:\n",
        "    print(\"\\nStacking Classifier performs better than the Voting Classifier.\")\n",
        "else:\n",
        "    print(\"\\nVoting Classifier performs better than the Stacking Classifier.\")\n",
        "\n",
        "Q17\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature importance scores\n",
        "feature_importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display the feature names and their importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(importance_df.head(5))\n",
        "\n",
        "Q18\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with DecisionTreeClassifier as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,  # Number of base estimators\n",
        "    max_samples=0.8,  # Fraction of samples to draw\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, average='weighted'),\n",
        "    'recall': make_scorer(recall_score, average='weighted'),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "# Extract and print mean and standard deviation of scores\n",
        "precision_mean = np.mean(cv_results['test_precision'])\n",
        "precision_std = np.std(cv_results['test_precision'])\n",
        "recall_mean = np.mean(cv_results['test_recall'])\n",
        "recall_std = np.std(cv_results['test_recall'])\n",
        "f1_mean = np.mean(cv_results['test_f1'])\n",
        "f1_std = np.std(cv_results['test_f1'])\n",
        "\n",
        "print(\"5-Fold Cross-Validation Results for Bagging Classifier:\")\n",
        "print(f\"Precision: Mean = {precision_mean:.4f}, Std = {precision_std:.4f}\")\n",
        "print(f\"Recall: Mean = {recall_mean:.4f}, Std = {recall_std:.4f}\")\n",
        "print(f\"F1-Score: Mean = {f1_mean:.4f}, Std = {f1_std:.4f}\")\n",
        "Q19\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of different max_depth values to try\n",
        "max_depth_values = range(1, 21)  # Trying depths from 1 to 20\n",
        "\n",
        "# List to store accuracy scores for each max_depth value\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train Random Forest models with different max_depth values\n",
        "for depth in max_depth_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Plot the accuracy as a function of max_depth\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depth_values, accuracy_scores, marker='o', color='b', linestyle='-', linewidth=2, markersize=6)\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(range(1, 21))\n",
        "plt.show()\n",
        "\n",
        "Q20\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = {\n",
        "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
        "    'KNeighbors': KNeighborsRegressor(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'mse': make_scorer(mean_squared_error, greater_is_better=False),\n",
        "    'r2': make_scorer(r2_score)\n",
        "}\n",
        "\n",
        "# Train and evaluate Bagging Regressor for each base estimator\n",
        "results = {}\n",
        "for name, estimator in base_estimators.items():\n",
        "    # Initialize Bagging Regressor\n",
        "    bagging = BaggingRegressor(\n",
        "        estimator=estimator,\n",
        "        n_estimators=50,  # Number of base estimators\n",
        "        max_samples=0.8,  # Fraction of samples to draw\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Perform 5-fold cross-validation\n",
        "    cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "    # Store mean and std of scores\n",
        "    results[name] = {\n",
        "        'mse_mean': -np.mean(cv_results['test_mse']),  # Negate because greater_is_better=False\n",
        "        'mse_std': np.std(cv_results['test_mse']),\n",
        "        'r2_mean': np.mean(cv_results['test_r2']),\n",
        "        'r2_std': np.std(cv_results['test_r2'])\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor Performance Comparison:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"\\nBase Estimator: {name}\")\n",
        "    print(f\"MSE: Mean = {metrics['mse_mean']:.4f}, Std = {metrics['mse_std']:.4f}\")\n",
        "    print(f\"R²: Mean = {metrics['r2_mean']:.4f}, Std = {metrics['r2_std']:.4f}\")\n",
        "Q21\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for each class\n",
        "y_pred_prob = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Binarize the output labels for multi-class ROC-AUC (One-vs-Rest approach)\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test_bin, y_pred_prob, average='macro', multi_class='ovr')\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "Q22\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with DecisionTreeClassifier as base estimator\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,  # Number of base estimators\n",
        "    max_samples=0.8,  # Fraction of samples to draw\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define scoring metrics\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1': make_scorer(f1_score, average='weighted')\n",
        "}\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_results = cross_validate(bagging, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
        "\n",
        "# Extract and print mean and standard deviation of scores\n",
        "accuracy_mean = np.mean(cv_results['test_accuracy'])\n",
        "accuracy_std = np.std(cv_results['test_accuracy'])\n",
        "f1_mean = np.mean(cv_results['test_f1'])\n",
        "f1_std = np.std(cv_results['test_f1'])\n",
        "\n",
        "print(f\"5-Fold Cross-Validation Results:\")\n",
        "print(f\"Accuracy: Mean = {accuracy_mean:.4f}, Std = {accuracy_std:.4f}\")\n",
        "print(f\"F1-Score: Mean = {f1_mean:.4f}, Std = {f1_std:.4f}\")\n",
        "Q23\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Convert the problem into a binary classification for Precision-Recall curve (class 0 vs rest)\n",
        "y_binary = (y == 0).astype(int)  # 1 if class 0, else 0\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate Precision and Recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Calculate Average Precision Score\n",
        "avg_precision = average_precision_score(y_test, y_pred_prob)\n",
        "print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', color='b', label=f'Random Forest (AP={avg_precision:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "Q24\n",
        "# Import necessary libraries\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the base models\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=200, random_state=42))\n",
        "]\n",
        "\n",
        "# Initialize the Stacking Classifier with Logistic Regression as the final estimator\n",
        "stacking_model = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_stacking = stacking_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Stacking Classifier\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "\n",
        "# Train and evaluate Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Train and evaluate Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=200, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "\n",
        "# Print the accuracy of each model\n",
        "print(f\"Accuracy of Stacking Classifier: {accuracy_stacking:.4f}\")\n",
        "print(f\"Accuracy of Random Forest Classifier: {accuracy_rf:.4f}\")\n",
        "print(f\"Accuracy of Logistic Regression: {accuracy_lr:.4f}\")\n",
        "\n",
        "Q25\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define different levels of max_samples\n",
        "max_samples_values = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressor for each max_samples value\n",
        "for max_samples in max_samples_values:\n",
        "    # Initialize Bagging Regressor with DecisionTreeRegressor as base estimator\n",
        "    bagging = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,  # Number of base estimators\n",
        "        max_samples=max_samples,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    bagging.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = bagging.predict(X_test)\n",
        "\n",
        "    # Calculate mean squared error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"max_samples={max_samples:.1f}, MSE={mse:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(max_samples_values, mse_scores, marker='o', linestyle='-', color='#1f77b4')\n",
        "plt.xlabel('max_samples')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Bagging Regressor Performance vs. max_samples')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ]
}